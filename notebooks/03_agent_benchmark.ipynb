{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 â€” Agent Benchmark\n",
    "\n",
    "Quick benchmark of PPO vs MCP-PPO vs DQN vs MAPPO on a short run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from config.simulation_config import SimulationConfig\n",
    "from simulation.environment import SwarmEnvironment\n",
    "from rl_agents.ppo_agent import PPOAgent\n",
    "from rl_agents.context_aware_agent import ContextAwareAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_ppo_baseline(episodes=5):\n",
    "    config = SimulationConfig()\n",
    "    config.render = False\n",
    "    env = SwarmEnvironment(config, mcp_server_url=None)\n",
    "    env.mcp_connected = False\n",
    "    state_dim = env.observation_space.shape[0] // config.num_uavs\n",
    "    action_dim = env.action_space.shape[0] // config.num_uavs\n",
    "    agents = [PPOAgent(f'ppo_{i}', state_dim, action_dim, {\n",
    "        'learning_rate': 3e-4, 'gamma': 0.99, 'batch_size': 64,\n",
    "        'buffer_size': 10000, 'action_scale': 2.0\n",
    "    }) for i in range(config.num_uavs)]\n",
    "    rewards = []\n",
    "    for ep in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        ep_rew = 0\n",
    "        while True:\n",
    "            acts = []\n",
    "            for i, a in enumerate(agents):\n",
    "                sd = len(obs) // len(agents)\n",
    "                acts.extend(a.select_action(obs[i*sd:(i+1)*sd]))\n",
    "            obs, r, term, trunc, info = env.step(np.array(acts))\n",
    "            ep_rew += r\n",
    "            if term or trunc:\n",
    "                break\n",
    "        rewards.append(ep_rew)\n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "rewards = asyncio.run(run_ppo_baseline(5))\n",
    "print('PPO baseline rewards:', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(rewards, 'o-', label='PPO Baseline')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Agent Benchmark (PPO)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / 'results' / '03_agent_benchmark.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
